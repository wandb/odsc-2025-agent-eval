{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f1d7c9",
   "metadata": {},
   "source": [
    "# ODSC 25 AI Agent Evaluation Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dac88c",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc7010",
   "metadata": {},
   "source": [
    "Install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f6a746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emmanuel.turlay/Code/odsc-2025-agent-eval/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai pandas pydantic weave --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9416820",
   "metadata": {},
   "source": [
    "Set environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e597bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "#os.environ[\"WANDB_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3876e",
   "metadata": {},
   "source": [
    "Sign up at https://wandb.com and go to https://wandb.ai/authorize to get your API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc194e",
   "metadata": {},
   "source": [
    "## Data analysis agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47140f",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddec045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List, Any, Union\n",
    "import pandas as pd\n",
    "import weave\n",
    "from openai import OpenAI\n",
    "from pydantic import Field\n",
    "import json\n",
    "\n",
    "class DataAnalysisAgent(weave.Model):\n",
    "\n",
    "    df: Optional[pd.DataFrame] = None\n",
    "\n",
    "    conversation_history: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "\n",
    "    client: OpenAI = Field(default_factory=lambda: OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")))\n",
    "\n",
    "    SYSTEM_PROMPT: weave.StringPrompt = Field(\n",
    "        default_factory=lambda: weave.StringPrompt(\"\"\"You are a data analysis assistant. You help users analyze datasets by using available tools.\n",
    "                \n",
    "When analyzing data:\n",
    "1. First load the dataset if not already loaded\n",
    "2. Understand what the user is asking\n",
    "3. Use appropriate tools to gather information\n",
    "4. Provide clear, accurate answers based on the data\n",
    "\n",
    "Always explain your findings clearly and relate them back to the user's question.\n",
    "\n",
    "Files are located in the data directory. For example, tips.csv is at data/tips.csv. Always use the correct file path.\n",
    "\"\"\")\n",
    "    )\n",
    "    \n",
    "    def model_post_init(self, __context: Any) -> None:\n",
    "        \"\"\"Called after the model is initialized\"\"\"\n",
    "        super().model_post_init(__context)\n",
    "        weave.publish(self.SYSTEM_PROMPT)\n",
    "\n",
    "    @property\n",
    "    def tool_registry(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"load_csv\": self.load_csv,\n",
    "            \"get_summary_statistics\": self.get_summary_statistics,\n",
    "            \"calculate_correlation\": self.calculate_correlation,\n",
    "            \"group_and_aggregate\": self.group_and_aggregate,\n",
    "            \"filter_data\": self.filter_data\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def tool_schemas(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Define the tools available to the agent\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"load_csv\",\n",
    "                    \"description\": \"Load a CSV file into memory for analysis\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"file_path\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Path to the CSV file\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"file_path\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_summary_statistics\",\n",
    "                    \"description\": \"Get summary statistics (mean, std, min, max, etc.) for numeric columns\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"columns\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"},\n",
    "                                \"description\": \"List of column names to analyze. If not provided, analyzes all numeric columns.\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"calculate_correlation\",\n",
    "                    \"description\": \"Calculate the correlation coefficient between two numeric columns\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"column1\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"First column name\"\n",
    "                            },\n",
    "                            \"column2\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Second column name\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"column1\", \"column2\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"group_and_aggregate\",\n",
    "                    \"description\": \"Group data by a column and calculate aggregate statistics\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"group_by\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Column to group by\"\n",
    "                            },\n",
    "                            \"agg_column\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Column to aggregate\"\n",
    "                            },\n",
    "                            \"agg_function\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"mean\", \"sum\", \"count\", \"median\"],\n",
    "                                \"description\": \"Aggregation function to apply\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"group_by\", \"agg_column\", \"agg_function\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"filter_data\",\n",
    "                    \"description\": \"Filter the dataset based on a condition\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"column\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Column to filter on\"\n",
    "                            },\n",
    "                            \"operator\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\">\", \"<\", \"==\", \">=\", \"<=\"],\n",
    "                                \"description\": \"Comparison operator\"\n",
    "                            },\n",
    "                            \"value\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"Value to compare against\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"column\", \"operator\", \"value\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    @weave.op\n",
    "    def load_csv(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load a CSV file into a pandas DataFrame\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(file_path)\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": f\"Loaded dataset with {len(self.df)} rows and {len(self.df.columns)} columns\",\n",
    "                \"columns\": list(self.df.columns),\n",
    "                \"shape\": self.df.shape,\n",
    "                \"head\": self.df.head(3).to_dict()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "    @weave.op\n",
    "    def get_summary_statistics(self, columns: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary statistics for specified columns or all numeric columns\"\"\"\n",
    "        if self.df is None:\n",
    "            return {\"status\": \"error\", \"message\": \"No dataset loaded\"}\n",
    "        \n",
    "        try:\n",
    "            if columns:\n",
    "                stats = self.df[columns].describe().to_dict()\n",
    "            else:\n",
    "                stats = self.df.describe().to_dict()\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"statistics\": stats\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "    @weave.op\n",
    "    def calculate_correlation(self, column1: str, column2: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate correlation between two columns\"\"\"\n",
    "        if self.df is None:\n",
    "            return {\"status\": \"error\", \"message\": \"No dataset loaded\"}\n",
    "        \n",
    "        try:\n",
    "            correlation = self.df[column1].corr(self.df[column2])\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"column1\": column1,\n",
    "                \"column2\": column2,\n",
    "                \"correlation\": float(correlation)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "    @weave.op\n",
    "    def group_and_aggregate(self, group_by: str, agg_column: str, agg_function: str = \"mean\") -> Dict[str, Any]:\n",
    "        \"\"\"Group by a column and aggregate another column\"\"\"\n",
    "        if self.df is None:\n",
    "            return {\"status\": \"error\", \"message\": \"No dataset loaded\"}\n",
    "        \n",
    "        try:\n",
    "            if agg_function == \"mean\":\n",
    "                result = self.df.groupby(group_by)[agg_column].mean()\n",
    "            elif agg_function == \"sum\":\n",
    "                result = self.df.groupby(group_by)[agg_column].sum()\n",
    "            elif agg_function == \"count\":\n",
    "                result = self.df.groupby(group_by)[agg_column].count()\n",
    "            elif agg_function == \"median\":\n",
    "                result = self.df.groupby(group_by)[agg_column].median()\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"message\": f\"Unsupported aggregation: {agg_function}\"}\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"group_by\": group_by,\n",
    "                \"agg_column\": agg_column,\n",
    "                \"agg_function\": agg_function,\n",
    "                \"result\": result.to_dict()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "    @weave.op\n",
    "    def filter_data(self, column: str, operator: str, value: Union[int, float, str]) -> Dict[str, Any]:\n",
    "        \"\"\"Filter the dataset based on a condition\"\"\"\n",
    "        if self.df is None:\n",
    "            return {\"status\": \"error\", \"message\": \"No dataset loaded\"}\n",
    "        \n",
    "        try:\n",
    "            if operator == \">\":\n",
    "                filtered = self.df[self.df[column] > value]\n",
    "            elif operator == \"<\":\n",
    "                filtered = self.df[self.df[column] < value]\n",
    "            elif operator == \"==\":\n",
    "                filtered = self.df[self.df[column] == value]\n",
    "            elif operator == \">=\":\n",
    "                filtered = self.df[self.df[column] >= value]\n",
    "            elif operator == \"<=\":\n",
    "                filtered = self.df[self.df[column] <= value]\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"message\": f\"Unsupported operator: {operator}\"}\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"rows_matched\": len(filtered),\n",
    "                \"sample\": filtered.head(5).to_dict()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "    @weave.op\n",
    "    def execute_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a tool and return the result\"\"\"\n",
    "        if tool_name in self.tool_registry:\n",
    "            return self.tool_registry[tool_name](**arguments)\n",
    "\n",
    "        return {\"status\": \"error\", \"message\": f\"Unknown tool: {tool_name}\"}\n",
    "    \n",
    "    @weave.op\n",
    "    def predict(self, query: str, max_iterations: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the agent on a query\n",
    "        Returns: (final_answer, execution_trace)\n",
    "        \"\"\"\n",
    "        # Initialize conversation\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.SYSTEM_PROMPT.format()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Track execution for evaluation\n",
    "        execution_trace = {\n",
    "            \"query\": query,\n",
    "            \"tool_calls\": [],\n",
    "            \"iterations\": 0\n",
    "        }\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            execution_trace[\"iterations\"] = iteration + 1\n",
    "            \n",
    "            # Get response from LLM\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                tools=self.tool_schemas,\n",
    "                tool_choice=\"auto\"\n",
    "            )\n",
    "            \n",
    "            message = response.choices[0].message\n",
    "            messages.append(message)\n",
    "            \n",
    "            # Check if we're done\n",
    "            if not message.tool_calls:\n",
    "                final_answer = message.content\n",
    "                execution_trace[\"final_answer\"] = final_answer\n",
    "                return {\n",
    "                    \"answer\": final_answer,\n",
    "                    \"execution_trace\": execution_trace\n",
    "                }\n",
    "            \n",
    "            # Execute tool calls\n",
    "            for tool_call in message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                arguments = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                # Execute the tool\n",
    "                result = self.execute_tool(tool_name, arguments)\n",
    "                \n",
    "                # Record tool call for evaluation\n",
    "                execution_trace[\"tool_calls\"].append({\n",
    "                    \"tool\": tool_name,\n",
    "                    \"arguments\": arguments,\n",
    "                    \"result\": result\n",
    "                })\n",
    "                \n",
    "                # Add tool result to conversation\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": json.dumps(result)\n",
    "                })\n",
    "        \n",
    "        # Max iterations reached\n",
    "        return {\n",
    "            \"answer\": \"Error: Maximum iterations reached\",\n",
    "            \"execution_trace\": execution_trace\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0575f9e",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582202b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üì¶ Published to https://wandb.ai/wandb/odsc-2025-agent-eval/weave/objects/StringPrompt/versions/F8OC8RdKNTFAetj7uJYkUJHhHXK7GSewQGrAK7Z8jeI\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb/odsc-2025-agent-eval/r/call/019a27ec-5ee2-7e60-9fc3-c48188275715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'The tips dataset has a total of **244 rows** and **7 columns**. If you need further analysis or information from this dataset, feel free to ask!',\n",
       " 'execution_trace': {'query': 'Load the tips dataset from tips.csv and tell me how many rows it has.',\n",
       "  'tool_calls': [{'tool': 'load_csv',\n",
       "    'arguments': {'file_path': 'data/tips.csv'},\n",
       "    'result': {'status': 'success',\n",
       "     'message': 'Loaded dataset with 244 rows and 7 columns',\n",
       "     'columns': ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size'],\n",
       "     'shape': (244, 7),\n",
       "     'head': {'total_bill': {0: 16.99, 1: 10.34, 2: 21.01},\n",
       "      'tip': {0: 1.01, 1: 1.66, 2: 3.5},\n",
       "      'sex': {0: 'Female', 1: 'Male', 2: 'Male'},\n",
       "      'smoker': {0: 'No', 1: 'No', 2: 'No'},\n",
       "      'day': {0: 'Sun', 1: 'Sun', 2: 'Sun'},\n",
       "      'time': {0: 'Dinner', 1: 'Dinner', 2: 'Dinner'},\n",
       "      'size': {0: 2, 1: 3, 2: 3}}}}],\n",
       "  'iterations': 2,\n",
       "  'final_answer': 'The tips dataset has a total of **244 rows** and **7 columns**. If you need further analysis or information from this dataset, feel free to ask!'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weave.init(\"odsc-2025-agent-eval\")\n",
    "agent = DataAnalysisAgent()\n",
    "agent.predict(\"Load the tips dataset from tips.csv and tell me how many rows it has.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4d503",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e382e",
   "metadata": {},
   "source": [
    "Evaluations are made of three components:\n",
    "\n",
    "* An evaluation dataset: a list of input prompts with associated ground truth.\n",
    "* A set of scorers: the logic to evaluate the generated outputs against the ground truth.\n",
    "* The agent to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5127be",
   "metadata": {},
   "source": [
    "### String-based evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2db554",
   "metadata": {},
   "source": [
    "A basic evaluation technique that searches for exact string matches in the agent's output.\n",
    "We don't expect this scorer to yield 100% pass scores, since LLMs can be fuzzy around math and rounding but we should keep a high threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "621f7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def exact_match_scorer(output: Dict[str, Any], expected_contains: str) -> Dict[str, bool]:\n",
    "    \"\"\"Score based on whether answer contains expected string\"\"\"\n",
    "    answer: str = output.get(\"answer\", \"\")\n",
    "    contains = expected_contains.lower() in answer.lower()\n",
    "    return {\"correct\": contains}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f515e40",
   "metadata": {},
   "source": [
    "### Numerical accuracy scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af372b96",
   "metadata": {},
   "source": [
    "This scorer will compare the actual numerical values and log the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99d4f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def numeric_accuracy_scorer(output: Dict[str, Any], ground_truth: float, tolerance: float = 0.1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract numeric value from answer and compare to ground truth\n",
    "    \"\"\"\n",
    "    answer = output.get(\"answer\", \"\")\n",
    "    \n",
    "    # Simple numeric extraction (you might need more sophisticated parsing)\n",
    "    import re\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', answer)\n",
    "    \n",
    "    if not numbers:\n",
    "        return {\"correct\": False, \"score\": 0.0, \"reason\": \"No numeric value found\"}\n",
    "    \n",
    "    # Take the first number found\n",
    "    for number in numbers:\n",
    "        extracted = float(number)\n",
    "        difference = abs(extracted - ground_truth)\n",
    "        if difference <= tolerance:\n",
    "            return {\n",
    "                \"correct\": True,\n",
    "                \"score\": 1.0,\n",
    "                \"extracted_value\": extracted,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"difference\": difference\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        \"correct\": False,\n",
    "        \"score\": 0.0,\n",
    "        \"extracted_value\": extracted,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"difference\": difference\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185cf56a",
   "metadata": {},
   "source": [
    "### The evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e129d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUND_TRUTH_DATASET = weave.Dataset(\n",
    "    name=\"Ground Truth Dataset\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"id\": \"avg_tip\",\n",
    "            \"query\": \"What is the average tip amount?\",\n",
    "            \"expected_contains\": \"2.99\",\n",
    "            \"ground_truth\": 2.99,\n",
    "            \"tolerance\": 0.1,\n",
    "            \"expected_tools\": [\"get_summary_statistics\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"avg_tip_percentage\",\n",
    "            \"query\": \"What is the average tip percentage?\",\n",
    "            \"expected_contains\": \"15.14\",\n",
    "            \"ground_truth\": 15.14,\n",
    "            \"tolerance\": 0.5,\n",
    "            \"expected_tools\": [\"get_summary_statistics\", \"group_and_aggregate\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"row_count\",\n",
    "            \"query\": \"How many rows are in the dataset?\",\n",
    "            \"expected_contains\": \"244\",\n",
    "            \"ground_truth\": 244,\n",
    "            \"tolerance\": 0,\n",
    "            \"expected_tools\": [\"load_csv\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"correlation\",\n",
    "            \"query\": \"What is the correlation between total_bill and tip?\",\n",
    "            \"expected_contains\": \"correlation\",\n",
    "            \"ground_truth\": 0.68,  # Approximate\n",
    "            \"tolerance\": 0.1,\n",
    "            \"expected_tools\": [\"calculate_correlation\"]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd90188",
   "metadata": {},
   "source": [
    "### Run the evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba57bfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üì¶ Published to https://wandb.ai/wandb/odsc-2025-agent-eval/weave/objects/StringPrompt/versions/F8OC8RdKNTFAetj7uJYkUJHhHXK7GSewQGrAK7Z8jeI\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb/odsc-2025-agent-eval/r/call/019a27ec-6937-71fe-88b2-e8959ba6168a\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb/odsc-2025-agent-eval/r/call/019a27ec-8552-75b4-b7fc-1ce7f9911fb5\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"output\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"execution_trace\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"iterations\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:         \"mean\": 3.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"exact_match_scorer\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correct\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 3,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.75\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"numeric_accuracy_scorer\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correct\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 4,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"extracted_value\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 65.7075\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"ground_truth\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 65.7025\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"difference\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.004999999999999893\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 7.895692229270935\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n"
     ]
    }
   ],
   "source": [
    "agent = DataAnalysisAgent()\n",
    "    \n",
    "# Load dataset first\n",
    "agent.predict(\"Load tips.csv\")\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    name=\"Ground Truth Evaluation\",\n",
    "    dataset=GROUND_TRUTH_DATASET,\n",
    "    scorers=[\n",
    "        exact_match_scorer,\n",
    "        numeric_accuracy_scorer,\n",
    "    ]\n",
    ")\n",
    "    \n",
    "# Run evaluation\n",
    "results = await evaluation.evaluate(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3c124",
   "metadata": {},
   "source": [
    "## Testing agentic behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00afd958",
   "metadata": {},
   "source": [
    "### Tool selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f61fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def tool_selection_scorer(output: Dict[str, Any], expected_tools: List[str], forbidden_tools: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Score based on whether correct tools were used\n",
    "    \"\"\"\n",
    "    tools_used = output.get(\"tools_used\", [])\n",
    "    forbidden_tools = forbidden_tools or []\n",
    "    \n",
    "    # Check if all expected tools were used\n",
    "    has_required = all(tool in tools_used for tool in expected_tools)\n",
    "    \n",
    "    # Check if any forbidden tools were used\n",
    "    has_forbidden = any(tool in tools_used for tool in forbidden_tools)\n",
    "    \n",
    "    correct = has_required and not has_forbidden\n",
    "    \n",
    "    return {\n",
    "        \"correct\": correct,\n",
    "        \"score\": 1.0 if correct else 0.0,\n",
    "        \"expected_tools\": expected_tools,\n",
    "        \"actual_tools\": tools_used,\n",
    "        \"has_required_tools\": has_required,\n",
    "        \"has_forbidden_tools\": has_forbidden\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cafa5f",
   "metadata": {},
   "source": [
    "### Trajectory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37db1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def efficiency_scorer(output: Dict[str, Any], max_iterations: int = 5, \n",
    "                     max_tool_calls: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Score based on execution efficiency\n",
    "    \"\"\"\n",
    "    iterations = output.get(\"iterations\", 0)\n",
    "    num_tools = output.get(\"num_tool_calls\", 0)\n",
    "    \n",
    "    efficient = iterations <= max_iterations and num_tools <= max_tool_calls\n",
    "    \n",
    "    # Score decreases with more iterations/tools\n",
    "    score = 1.0\n",
    "    if iterations > max_iterations:\n",
    "        score *= (max_iterations / iterations)\n",
    "    if num_tools > max_tool_calls:\n",
    "        score *= (max_tool_calls / num_tools)\n",
    "    \n",
    "    return {\n",
    "        \"correct\": efficient,\n",
    "        \"score\": score,\n",
    "        \"iterations\": iterations,\n",
    "        \"tool_calls\": num_tools,\n",
    "        \"efficient\": efficient\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751181ef",
   "metadata": {},
   "source": [
    "### Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5304c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_SELECTION_DATASET = weave.Dataset(\n",
    "    name=\"Tool Selection Dataset\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"id\": \"correlation_test\",\n",
    "            \"query\": \"What is the correlation between total_bill and tip?\",\n",
    "            \"expected_tools\": [\"calculate_correlation\"],\n",
    "            \"forbidden_tools\": [\"filter_data\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"statistics_test\",\n",
    "            \"query\": \"Show me statistics for the tip column\",\n",
    "            \"expected_tools\": [\"get_summary_statistics\"],\n",
    "            \"forbidden_tools\": [\"calculate_correlation\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"groupby_test\",\n",
    "            \"query\": \"What's the average tip by day of week?\",\n",
    "            \"expected_tools\": [\"group_and_aggregate\"],\n",
    "            \"forbidden_tools\": [\"filter_data\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"comparison_test\",\n",
    "            \"query\": \"Compare average tips between smokers and non-smokers\",\n",
    "            \"expected_tools\": [\"group_and_aggregate\"],\n",
    "            \"forbidden_tools\": []\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcaa94a",
   "metadata": {},
   "source": [
    "### Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c1568f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üì¶ Published to https://wandb.ai/wandb/odsc-2025-agent-eval/weave/objects/StringPrompt/versions/F8OC8RdKNTFAetj7uJYkUJHhHXK7GSewQGrAK7Z8jeI\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb/odsc-2025-agent-eval/r/call/019a27ec-cb85-7ee8-bc09-87ff4c8b142c\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb/odsc-2025-agent-eval/r/call/019a27ec-df54-70be-beaf-356b2528dac8\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"output\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"execution_trace\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"iterations\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:         \"mean\": 3.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"tool_selection_scorer\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correct\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 0,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"has_required_tools\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 0,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"has_forbidden_tools\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 0,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"efficiency_scorer\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correct\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 4,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"iterations\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"tool_calls\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"efficient\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 4,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 6.697023093700409\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n"
     ]
    }
   ],
   "source": [
    "agent = DataAnalysisAgent()\n",
    "agent.predict(\"Load tips.csv\")\n",
    "    \n",
    "# Create Weave evaluation\n",
    "evaluation = weave.Evaluation(\n",
    "    name=\"Tool Selection Evaluation\",\n",
    "    dataset=TOOL_SELECTION_DATASET,\n",
    "    scorers=[\n",
    "        tool_selection_scorer,\n",
    "        efficiency_scorer,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "results = await evaluation.evaluate(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bfd46",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation ‚Äì LLM-as-a-judge scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53d43cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def llm_judge_scorer(output: Dict[str, Any], query: str, ground_truth: Any = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use GPT-4 as a judge to evaluate answer quality\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    answer = output.get(\"answer\", \"\")\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"You are evaluating a data analysis agent's response.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Agent's Answer: {answer}\n",
    "\n",
    "Ground Truth Data (for reference): {ground_truth if ground_truth is not None else \"Not provided\"}\n",
    "\n",
    "Evaluate the answer on these criteria (score 1-5 for each):\n",
    "1. ACCURACY: Is the numerical information correct?\n",
    "2. COMPLETENESS: Does it fully answer the question?\n",
    "3. CLARITY: Is the explanation clear and well-structured?\n",
    "4. RELEVANCE: Does it stay focused on the question?\n",
    "\n",
    "Provide scores and brief justification in JSON format:\n",
    "{{\n",
    "    \"accuracy\": <1-5>,\n",
    "    \"completeness\": <1-5>,\n",
    "    \"clarity\": <1-5>,\n",
    "    \"relevance\": <1-5>,\n",
    "    \"justification\": \"<explanation>\",\n",
    "    \"overall_pass\": <true/false>\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": evaluation_prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        evaluation = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Calculate average score (1-5 scale normalized to 0-1)\n",
    "        avg_score = (\n",
    "            evaluation[\"accuracy\"] + \n",
    "            evaluation[\"completeness\"] + \n",
    "            evaluation[\"clarity\"] + \n",
    "            evaluation[\"relevance\"]\n",
    "        ) / 4.0 / 5.0  # Normalize to 0-1\n",
    "        \n",
    "        return {\n",
    "            \"correct\": evaluation[\"overall_pass\"],\n",
    "            \"score\": avg_score,\n",
    "            **evaluation\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"correct\": False,\n",
    "            \"score\": 0.0,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45dbcd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üì¶ Published to https://wandb.ai/wandb/odsc-2025-agent-eval/weave/objects/StringPrompt/versions/F8OC8RdKNTFAetj7uJYkUJHhHXK7GSewQGrAK7Z8jeI\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb/odsc-2025-agent-eval/r/call/019a27ed-034c-7161-8e27-c5ecd5ff5522\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/wandb/odsc-2025-agent-eval/r/call/019a27ed-0fab-7567-823c-172029c8bfcf\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"output\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"execution_trace\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"iterations\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:         \"mean\": 4.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"llm_judge_scorer\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correct\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 2,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.5\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.8500000000000001\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"accuracy\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 3.5\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"completeness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 4.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"clarity\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 4.5\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"relevance\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 5.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"overall_pass\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 2,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.5\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 10.839269518852234\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n"
     ]
    }
   ],
   "source": [
    "agent = DataAnalysisAgent()\n",
    "agent.predict(\"Load tips.csv\")\n",
    "\n",
    "# Create Weave evaluation\n",
    "evaluation = weave.Evaluation(\n",
    "    name=\"LLM-as-Judge Evaluation\",\n",
    "    dataset=GROUND_TRUTH_DATASET,\n",
    "    scorers=[\n",
    "        llm_judge_scorer,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "results = await evaluation.evaluate(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
